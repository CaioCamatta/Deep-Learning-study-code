{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand A2C implementation playing Sonic the Hedgehog ü¶î\n",
    "\n",
    "This Notebook explains the Advantage Actor Critic implementation.<br>\n",
    "The [repository link](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/A2C%20with%20Sonic%20the%20Hedgehog) \n",
    "\n",
    "### Acknowledgements üëè\n",
    "This implementation was inspired by 2 repositories:\n",
    "- OpenAI [Baselines A2C](https://github.com/openai/baselines/tree/master/baselines/a2c)\n",
    "- Alexandre Borghi [retro_contest_agent](https://github.com/aborghi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/sonic.gif\" alt=\"Sonic the hedgehog Deep reinforcement learning course\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use it? üìñ\n",
    "‚ö†Ô∏è First you need to follow Step 1 (Download Sonic the Hedgehog)\n",
    "### Watch our agent playing üëÄ\n",
    "- **Modify the environment**: change `env.make_train_3`with the env you want in `env= DummyVecEnv([env.make_train_3]))` in play.py\n",
    "- **See the agent playing**: run `python play.py`\n",
    "\n",
    "### Continue to train üèÉ‚Äç‚ôÇÔ∏è\n",
    "‚ö†Ô∏è There is a big risk **of overfitting**\n",
    "- Run `python agent.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Sonic the Hedgehog üíª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first step is to download the game, to make it works on retro **you need to buy it legally on [Steam](https://store.steampowered.com/app/71113/Sonic_The_Hedgehog/)**\n",
    "<br>\n",
    "<img src=\"https://steamcdn-a.akamaihd.net/steam/apps/71113/header.jpg?t=1495556871\" alt=\"Sonic the Hedgehog\"/>\n",
    "<br>\n",
    "2. Then follow the **Quickstart part** of this [website](https://contest.openai.com/2018-1/details/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build all elements we need for our environement in sonic_env.py üñºÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `PreprocessFrame(gym.ObservationWrapper)` : in this class we will **preprocess our environment**\n",
    "    - Set frame to gray \n",
    "    - Resize the frame to 96x96x1\n",
    "<br>\n",
    "<br>\n",
    "- `ActionsDiscretizer(gym.ActionWrapper)` : in this class we **limit the possibles actions in our environment** (make it discrete)\n",
    "\n",
    "In fact you'll see that for each action in actions:\n",
    "    Create an array of 12 False (12 = nb of buttons)\n",
    "        For each button in action: (for instance ['LEFT']) we need to make that left button index = True\n",
    "            Then the button index = LEFT = True\n",
    "\n",
    "--> In fact at the end we will have an array where each array is an action and **each elements True of this array are the buttons clicked.**\n",
    "For instance LEFT action = [F, F, F, F, F, F, T, F, F, F, F, F]\n",
    "<br><br>\n",
    "- `RewardScaler(gym.RewardWrapper)`: We **scale the rewards** to reasonable scale (useful in PPO but also in A2C).\n",
    "<br><br>\n",
    "- `AllowBacktracking(gym.Wrapper)`: **Allow the agent to go backward without being discourage** (avoid our agent to be stuck on a wall during the game).\n",
    "<br><br>\n",
    "\n",
    "- `make_env(env_idx)` : **Build an environement** (and stack 4 frames together using `FrameStack`\n",
    "<br><br>\n",
    "The idea is that we'll build multiple instances of the environment, different environments each times (different level) **to avoid overfitting and helping our agent to generalize better at playing sonic** \n",
    "<br><br>\n",
    "To handle these multiple environements we'll use `SubprocVecEnv` that **creates a vector of n environments to run them simultaneously.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part taken from adborghi fantastic implementation\n",
    "# https://github.com/aborghi/retro_contest_agent/blob/master/fastlearner/ppo2ttifrutti_sonic_env.py\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import model\n",
    "import architecture as policies\n",
    "import sonic_env as env\n",
    "\n",
    "#import gym_remote.client as grc\n",
    "\n",
    "from retro_contest.local import make\n",
    "from retro import make as make_retro\n",
    "\n",
    "# This will be useful for stacking frames\n",
    "from baselines.common.atari_wrappers import FrameStack\n",
    "\n",
    "# Library used to modify frames (former times we used matplotlib)\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "from baselines import logger\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate cross entropy\n",
    "from baselines.a2c.utils import cat_entropy, mse\n",
    "\n",
    "from baselines.common import explained_variance\n",
    "from baselines.common.runners import AbstractEnvRunner\n",
    "\n",
    "\n",
    "# SubprocVecEnv creates a vector of n environments to run them simultaneously.\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "\n",
    "# This function selects the probability distribution over actions\n",
    "from baselines.common.distributions import make_pdtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setUseOpenCL = False means that we will not use GPU (disable OpenCL acceleration)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Here we do the preprocessing part:\n",
    "    - Set frame to gray\n",
    "    - Resize the frame to 96x96x1\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 96\n",
    "        self.height = 96\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        # Set frame to gray\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Resize the frame to 96x96x1\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        frame = frame[:, :, None]\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "class ActionsDiscretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ActionsDiscretizer, self).__init__(env)\n",
    "        buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "\n",
    "        \"\"\"\n",
    "        What we do in this loop:\n",
    "        For each action in actions\n",
    "            - Create an array of 12 False (12 = nb of buttons)\n",
    "            For each button in action: (for instance ['LEFT']) we need to make that left button index = True\n",
    "                - Then the button index = LEFT = True\n",
    "            In fact at the end we will have an array where each array is an action and each elements True of this array\n",
    "            are the buttons clicked.\n",
    "        \"\"\"\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * 12)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()\n",
    "\n",
    "\n",
    "class RewardScaler(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Bring rewards to a reasonable scale for PPO.\n",
    "    This is incredibly important and effects performance\n",
    "    drastically.\n",
    "    \"\"\"\n",
    "    def reward(self, reward):\n",
    "\n",
    "        return reward * 0.01\n",
    "\n",
    "class AllowBacktracking(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Use deltas in max(X) as the reward, rather than deltas\n",
    "    in X. This way, agents are not discouraged too heavily\n",
    "    from exploring backwards if there is no way to advance\n",
    "    head-on in the level.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(AllowBacktracking, self).__init__(env)\n",
    "        self._cur_x = 0\n",
    "        self._max_x = 0\n",
    "\n",
    "    def reset(self, **kwargs): # pylint: disable=E0202\n",
    "        self._cur_x = 0\n",
    "        self._max_x = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action): # pylint: disable=E0202\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        self._cur_x += rew\n",
    "        rew = max(0, self._cur_x - self._max_x)\n",
    "        self._max_x = max(self._max_x, self._cur_x)\n",
    "        return obs, rew, done, info\n",
    "\n",
    "def make_env(env_idx):\n",
    "    \"\"\"\n",
    "    Create an environment with some standard wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    dicts = [\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'SpringYardZone.Act3'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'SpringYardZone.Act2'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'GreenHillZone.Act3'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'GreenHillZone.Act1'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'StarLightZone.Act2'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'StarLightZone.Act1'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'MarbleZone.Act2'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'MarbleZone.Act1'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'MarbleZone.Act3'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'ScrapBrainZone.Act2'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'LabyrinthZone.Act2'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'LabyrinthZone.Act1'},\n",
    "        {'game': 'SonicTheHedgehog-Genesis', 'state': 'LabyrinthZone.Act3'}\n",
    "        \n",
    "\n",
    "    ]\n",
    "    # Make the environment\n",
    "    print(dicts[env_idx]['game'], dicts[env_idx]['state'], flush=True)\n",
    "    #record_path = \"./records/\" + dicts[env_idx]['state']\n",
    "    env = make(game=dicts[env_idx]['game'], state=dicts[env_idx]['state'], bk2dir=\"./records\")#record='/tmp')\n",
    "\n",
    "    # Build the actions array, \n",
    "    env = ActionsDiscretizer(env)\n",
    "\n",
    "    # Scale the rewards\n",
    "    env = RewardScaler(env)\n",
    "\n",
    "    # PreprocessFrame\n",
    "    env = PreprocessFrame(env)\n",
    "\n",
    "    # Stack 4 frames\n",
    "    env = FrameStack(env, 4)\n",
    "\n",
    "    # Allow back tracking that helps agents are not discouraged too heavily\n",
    "    # from exploring backwards if there is no way to advance\n",
    "    # head-on in the level.\n",
    "    env = AllowBacktracking(env)\n",
    "\n",
    "\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "def make_train_0():\n",
    "    return make_env(0)\n",
    "\n",
    "def make_train_1():\n",
    "    return make_env(1)\n",
    "\n",
    "def make_train_2():\n",
    "    return make_env(2)\n",
    "\n",
    "def make_train_3():\n",
    "    return make_env(3)\n",
    "\n",
    "def make_train_4():\n",
    "    return make_env(4)\n",
    "\n",
    "def make_train_5():\n",
    "    return make_env(5)\n",
    "\n",
    "def make_train_6():\n",
    "    return make_env(6)\n",
    "\n",
    "def make_train_7():\n",
    "    return make_env(7)\n",
    "\n",
    "def make_train_8():\n",
    "    return make_env(8)\n",
    "\n",
    "def make_train_9():\n",
    "    return make_env(9)\n",
    "\n",
    "def make_train_10():\n",
    "    return make_env(10)\n",
    "\n",
    "def make_train_11():\n",
    "    return make_env(11)\n",
    "\n",
    "def make_train_12():\n",
    "    return make_env(12)\n",
    "\n",
    "def make_test_level_Green():\n",
    "    return make_test()\n",
    "\n",
    "\n",
    "def make_test():\n",
    "    \"\"\"\n",
    "    Create an environment with some standard wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Make the environment\n",
    "    env = make_retro(game='SonicTheHedgehog-Genesis', state='GreenHillZone.Act2', record=\"./records\")\n",
    "\n",
    "    # Build the actions array\n",
    "    env = ActionsDiscretizer(env)\n",
    "\n",
    "    # Scale the rewards\n",
    "    env = RewardScaler(env)\n",
    "\n",
    "    # PreprocessFrame\n",
    "    env = PreprocessFrame(env)\n",
    "\n",
    "    # Stack 4 frames\n",
    "    env = FrameStack(env, 4)\n",
    "\n",
    "    # Allow back tracking that helps agents are not discouraged too heavily\n",
    "    # from exploring backwards if there is no way to advance\n",
    "    # head-on in the level.\n",
    "    env = AllowBacktracking(env)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the A2C architecture in architecture.py üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `from baselines.common.distributions import make_pdtype`: This function selects the **probability distribution over actions**\n",
    "<br><br>\n",
    "\n",
    "- First, we create two functions that will help us to avoid to call conv and fc each time.\n",
    "    - `conv`: function to create a convolutional layer.\n",
    "    - `fc`: function to create a fully connected layer.\n",
    "<br><br>\n",
    "- Then, we create `A2CPolicy`, the object that **contains the architecture**<br>\n",
    "3 CNN for spatial dependencies<br>\n",
    "Temporal dependencies is handle by stacking frames<br>\n",
    "(Something funny nobody use LSTM in OpenAI Retro contest)<br>\n",
    "1 common FC<br>\n",
    "1 FC for policy<br>\n",
    "1 FC for value\n",
    "<br><br>\n",
    "- `self.pdtype = make_pdtype(action_space)`: Based on the action space, will **select what probability distribution typewe will use to distribute action in our stochastic policy** (in our case DiagGaussianPdType aka Diagonal Gaussian, multivariate normal distribution\n",
    "<br><br>\n",
    "- `self.pdtype.pdfromlatent` : return a returns a probability distribution over actions (self.pd) and our pi logits (self.pi).\n",
    "<br><br>\n",
    "\n",
    "- We create also 3 useful functions in A2CPolicy\n",
    "    - `def step(state_in, *_args, **_kwargs)`: Function use to take a step returns **action to take and V(s)**\n",
    "    - `def value(state_in, *_args, **_kwargs)`: Function that calculates **only the V(s)**\n",
    "    -  `def select_action(state_in, *_args, **_kwargs)`: Function that output **only the action to take**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution layer\n",
    "def conv_layer(inputs, filters, kernel_size, strides, gain=1.0):\n",
    "    return tf.layers.conv2d(inputs=inputs,\n",
    "                            filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=(strides, strides),\n",
    "                            activation=tf.nn.relu,\n",
    "                            kernel_initializer=tf.orthogonal_initializer(gain=gain))\n",
    "\n",
    "\n",
    "# Fully connected layer\n",
    "def fc_layer(inputs, units, activation_fn=tf.nn.relu, gain=1.0):\n",
    "    return tf.layers.dense(inputs=inputs,\n",
    "                           units=units,\n",
    "                           activation=activation_fn,\n",
    "                           kernel_initializer=tf.orthogonal_initializer(gain))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This object creates the A2C Network architecture\n",
    "\"\"\"\n",
    "class A2CPolicy(object):\n",
    "    def __init__(self, sess, ob_space, action_space, nbatch, nsteps, reuse = False):\n",
    "        # This will use to initialize our kernels\n",
    "        gain = np.sqrt(2)\n",
    "\n",
    "        # Based on the action space, will select what probability distribution type\n",
    "        # we will use to distribute action in our stochastic policy (in our case DiagGaussianPdType\n",
    "        # aka Diagonal Gaussian, 3D normal distribution\n",
    "        self.pdtype = make_pdtype(action_space)\n",
    "\n",
    "        height, weight, channel = ob_space.shape\n",
    "        ob_shape = (height, weight, channel)\n",
    "\n",
    "        # Create the input placeholder\n",
    "        inputs_ = tf.placeholder(tf.float32, [None, *ob_shape], name=\"input\")\n",
    "\n",
    "        # Normalize the images\n",
    "        scaled_images = tf.cast(inputs_, tf.float32) / 255.\n",
    "\n",
    "        \"\"\"\n",
    "        Build the model\n",
    "        3 CNN for spatial dependencies\n",
    "        Temporal dependencies is handle by stacking frames\n",
    "        (Something funny nobody use LSTM in OpenAI Retro contest)\n",
    "        1 common FC\n",
    "        1 FC for policy\n",
    "        1 FC for value\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"model\", reuse = reuse):\n",
    "            conv1 = conv_layer(scaled_images, 32, 8, 4, gain)\n",
    "            conv2 = conv_layer(conv1, 64, 4, 2, gain)\n",
    "            conv3 = conv_layer(conv2, 64, 3, 1, gain)\n",
    "            flatten1 = tf.layers.flatten(conv3)\n",
    "            fc_common = fc_layer(flatten1, 512, gain=gain)\n",
    "\n",
    "            # This build a fc connected layer that returns a probability distribution\n",
    "            # over actions (self.pd) and our pi logits (self.pi).\n",
    "            self.pd, self.pi = self.pdtype.pdfromlatent(fc_common, init_scale=0.01)\n",
    "\n",
    "            # Calculate the v(s)\n",
    "            vf = fc_layer(fc_common, 1, activation_fn=None)[:, 0]\n",
    "\n",
    "        self.initial_state = None\n",
    "\n",
    "        # Take an action in the action distribution (remember we are in a situation\n",
    "        # of stochastic policy so we don't always take the action with the highest probability\n",
    "        # for instance if we have 2 actions 0.7 and 0.3 we have 30% chance to take the second)\n",
    "        a0 = self.pd.sample()\n",
    "\n",
    "        # Function use to take a step returns action to take and V(s)\n",
    "        def step(state_in, *_args, **_kwargs):\n",
    "            action, value = sess.run([a0, vf], {inputs_: state_in})\n",
    "           \n",
    "            #print(\"step\", action)\n",
    "            \n",
    "            return action, value\n",
    "\n",
    "        # Function that calculates only the V(s)\n",
    "        def value(state_in, *_args, **_kwargs):\n",
    "            return sess.run(vf, {inputs_: state_in})\n",
    "\n",
    "        # Function that output only the action to take\n",
    "        def select_action(state_in, *_args, **_kwargs):\n",
    "            return sess.run(a0, {inputs_: state_in})\n",
    "\n",
    "        self.inputs_ = inputs_\n",
    "        self.vf = vf\n",
    "        self.step = step\n",
    "        self.value = value\n",
    "        self.select_action = select_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Model in model.py üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Model object to:\n",
    "__init__:\n",
    "- `policy(sess, ob_space, action_space, nenvs, 1, reuse=False)`: Creates the step_model (used for sampling)\n",
    "- `policy(sess, ob_space, action_space, nenvs*nsteps, nsteps, reuse=True)`: Creates the train_model (used for training)\n",
    "\n",
    "- `def train(states_in, actions, returns, values, lr)`: Make the training part (calculate advantage and feedforward and retropropagation of gradients)\n",
    "\n",
    "save/load():\n",
    "- `def save(save_path)`:Save the weights.\n",
    "- `def load(load_path)`:Load the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the variables\n",
    "def find_trainable_variables(key):\n",
    "    with tf.variable_scope(key):\n",
    "        return tf.trainable_variables()\n",
    "\n",
    "\n",
    "# Make directory\n",
    "def make_path(f):\n",
    "    # exist_ok: if the folder already exist makes no exception error\n",
    "    return os.makedirs(f, exist_ok=True)\n",
    "\n",
    "\n",
    "def discount_with_dones(rewards, dones, gamma):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        r = reward + gamma*r*(1.-done) # fixed off by one bug\n",
    "        discounted.append(r)\n",
    "    return discounted[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "    We use this object to :\n",
    "    __init__:\n",
    "    - Creates the step_model\n",
    "    - Creates the train_model\n",
    "    train():\n",
    "    - Make the training part (feedforward and retropropagation of gradients)\n",
    "    save/load():\n",
    "    - Save load the model\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 policy,\n",
    "                ob_space,\n",
    "                action_space,\n",
    "                nenvs,\n",
    "                nsteps,\n",
    "                ent_coef,\n",
    "                vf_coef,\n",
    "                max_grad_norm):\n",
    "\n",
    "        sess = tf.get_default_session()\n",
    "\n",
    "        # Here we create the placeholders\n",
    "        actions_ = tf.placeholder(tf.int32, [None], name=\"actions_\")\n",
    "        advantages_ = tf.placeholder(tf.float32, [None], name=\"advantages_\")\n",
    "        rewards_ = tf.placeholder(tf.float32, [None], name=\"rewards_\")\n",
    "        lr_ = tf.placeholder(tf.float32, name=\"learning_rate_\")\n",
    "\n",
    "        # Here we create our two models:\n",
    "        # Step_model that is used for sampling\n",
    "        step_model = policy(sess, ob_space, action_space, nenvs, 1, reuse=False)\n",
    "\n",
    "        # Train model for training\n",
    "        train_model = policy(sess, ob_space, action_space, nenvs*nsteps, nsteps, reuse=True)\n",
    "\n",
    "        \"\"\"\n",
    "        Calculate the loss\n",
    "        Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n",
    "        \"\"\"\n",
    "        # Policy loss\n",
    "        # Output -log(pi)\n",
    "        neglogpac = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.pi, labels=actions_)\n",
    "\n",
    "        # 1/n * sum A(si,ai) * -logpi(ai|si)\n",
    "        pg_loss = tf.reduce_mean(advantages_ * neglogpac)\n",
    "\n",
    "        # Value loss 1/2 SUM [R - V(s)]^2\n",
    "        vf_loss = tf.reduce_mean(mse(tf.squeeze(train_model.vf),rewards_))\n",
    "\n",
    "        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n",
    "        entropy = tf.reduce_mean(train_model.pd.entropy())\n",
    "\n",
    "\n",
    "        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n",
    "\n",
    "        # Update parameters using loss\n",
    "        # 1. Get the model parameters\n",
    "        params = find_trainable_variables(\"model\")\n",
    "\n",
    "        # 2. Calculate the gradients\n",
    "        grads = tf.gradients(loss, params)\n",
    "        if max_grad_norm is not None:\n",
    "            # Clip the gradients (normalize)\n",
    "            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "        grads = list(zip(grads, params))\n",
    "        # zip aggregate each gradient with parameters associated\n",
    "        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n",
    "\n",
    "        # 3. Build our trainer\n",
    "        trainer = tf.train.RMSPropOptimizer(learning_rate=lr_, decay=0.99, epsilon=1e-5)\n",
    "\n",
    "        # 4. Backpropagation\n",
    "        _train = trainer.apply_gradients(grads)\n",
    "\n",
    "        def train(states_in, actions, returns, values, lr):\n",
    "            # Here we calculate advantage A(s,a) = R + yV(s') - V(s)\n",
    "            # Returns = R + yV(s')\n",
    "            advantages = returns - values\n",
    "\n",
    "            # We create the feed dictionary\n",
    "            td_map = {train_model.inputs_: states_in,\n",
    "                     actions_: actions,\n",
    "                     advantages_: advantages, # Use to calculate our policy loss\n",
    "                     rewards_: returns, # Use as a bootstrap for real value\n",
    "                     lr_: lr}\n",
    "\n",
    "            policy_loss, value_loss, policy_entropy, _= sess.run([pg_loss, vf_loss, entropy, _train], td_map)\n",
    "            \n",
    "            return policy_loss, value_loss, policy_entropy\n",
    "\n",
    "\n",
    "        def save(save_path):\n",
    "            \"\"\"\n",
    "            Save the model\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_path)\n",
    "\n",
    "        def load(load_path):\n",
    "            \"\"\"\n",
    "            Load the model\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "            print('Loading ' + load_path)\n",
    "            saver.restore(sess, load_path)\n",
    "\n",
    "        self.train = train\n",
    "        self.train_model = train_model\n",
    "        self.step_model = step_model\n",
    "        self.step = step_model.step\n",
    "        self.value = step_model.value\n",
    "        self.initial_state = step_model.initial_state\n",
    "        self.save = save\n",
    "        self.load = load\n",
    "        tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Runner in model.py üèÉ\n",
    "Runner will be used to make a mini batch of experiences\n",
    "- Each environement send 1 timestep (4 frames stacked) (`self.obs`)\n",
    "- This goes through `step_model`\n",
    "    - Returns actions, values.\n",
    "- Append `mb_obs`, `mb_actions`, `mb_values`, `mb_dones`.\n",
    "- Take actions in environments and watch the consequences\n",
    "    - return `obs`, `rewards`, `dones`\n",
    "- We need to calculate advantage to do that we use General Advantage Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(AbstractEnvRunner):\n",
    "    \"\"\"\n",
    "    We use this object to make a mini batch of experiences\n",
    "    __init__:\n",
    "    - Initialize the runner\n",
    "    run():\n",
    "    - Make a mini batch\n",
    "    \"\"\"\n",
    "    def __init__(self, env, model, nsteps, total_timesteps, gamma, lam):\n",
    "        super().__init__(env = env, model = model, nsteps = nsteps)\n",
    "\n",
    "        # Discount rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Lambda used in GAE (General Advantage Estimation)\n",
    "        self.lam = lam\n",
    "\n",
    "        # Total timesteps taken\n",
    "        self.total_timesteps = total_timesteps\n",
    "\n",
    "    def run(self):\n",
    "        # Here, we init the lists that will contain the mb of experiences\n",
    "        mb_obs, mb_actions, mb_rewards, mb_values, mb_dones = [],[],[],[],[]\n",
    "\n",
    "        # For n in range number of steps\n",
    "        for n in range(self.nsteps):\n",
    "            # Given observations, take action and value (V(s))\n",
    "            # We already have self.obs because AbstractEnvRunner run self.obs[:] = env.reset()\n",
    "            actions, values = self.model.step(self.obs, self.dones)\n",
    "\n",
    "            #print(\"actions runner runner\", actions)\n",
    "\n",
    "            # Append the observations into the mb\n",
    "            mb_obs.append(np.copy(self.obs)) #obs len nenvs (1 step per env)\n",
    "\n",
    "            # Append the actions taken into the mb\n",
    "            mb_actions.append(actions)\n",
    "\n",
    "            # Append the values calculated into the mb\n",
    "            mb_values.append(values)\n",
    "\n",
    "            # Append the dones situations into the mb\n",
    "            mb_dones.append(self.dones)\n",
    "\n",
    "            # Take actions in env and look the results\n",
    "            self.obs[:], rewards, self.dones, _ = self.env.step(actions)\n",
    "\n",
    "            mb_rewards.append(rewards)\n",
    "\n",
    "        #batch of steps to batch of rollouts\n",
    "        mb_obs = np.asarray(mb_obs, dtype=np.uint8)\n",
    "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
    "        mb_actions = np.asarray(mb_actions, dtype=np.int32)\n",
    "        mb_values = np.asarray(mb_values, dtype=np.float32)\n",
    "        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
    "        last_values = self.model.value(self.obs)\n",
    "          \n",
    "\n",
    "        ### GENERALIZED ADVANTAGE ESTIMATION\n",
    "        # discount/bootstrap off value fn\n",
    "        # We create mb_returns and mb_advantages\n",
    "        # mb_returns will contain Advantage + value\n",
    "        mb_returns = np.zeros_like(mb_rewards)\n",
    "        mb_advantages = np.zeros_like(mb_rewards)\n",
    "\n",
    "        lastgaelam = 0\n",
    "\n",
    "        # From last step to first step\n",
    "        for t in reversed(range(self.nsteps)):\n",
    "            # If t == before last step\n",
    "            if t == self.nsteps - 1:\n",
    "                # If a state is done, nextnonterminal = 0\n",
    "                # In fact nextnonterminal allows us to do that logic\n",
    "\n",
    "                #if done (so nextnonterminal = 0):\n",
    "                #    delta = R - V(s) (because self.gamma * nextvalues * nextnonterminal = 0) \n",
    "                # else (not done)\n",
    "                    #delta = R + gamma * V(st+1)\n",
    "                nextnonterminal = 1.0 - self.dones\n",
    "                \n",
    "                # V(t+1)\n",
    "                nextvalues = last_values\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - mb_dones[t+1]\n",
    "                \n",
    "                nextvalues = mb_values[t+1]\n",
    "\n",
    "            # Delta = R(st) + gamma * V(t+1) * nextnonterminal  - V(st)\n",
    "            delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\n",
    "\n",
    "            # Advantage = delta + gamma *  Œª (lambda) * nextnonterminal  * lastgaelam\n",
    "            mb_advantages[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n",
    "\n",
    "        # Returns\n",
    "        mb_returns = mb_advantages + mb_values\n",
    "\n",
    "        return map(sf01, (mb_obs, mb_actions, mb_returns, mb_values))\n",
    "\n",
    "\n",
    "def sf01(arr):\n",
    "    \"\"\"\n",
    "    swap and then flatten axes 0 and 1\n",
    "    \"\"\"\n",
    "    s = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build the learn function in model.py\n",
    "The learn function can be seen as **the gathering of all the logic of our A2C**\n",
    "- Instantiate the model object (that creates step_model and train_model)\n",
    "- Instantiate the runner object\n",
    "- Train always in two phases:\n",
    "    - Run to get a batch of experiences\n",
    "    - Train that batch of experiences\n",
    "<br><br>\n",
    "\n",
    "We use explained_variance which is a **really important parameter**:\n",
    "<br><br>\n",
    "`ev = 1 - Variance[y - ypredicted] / Variance [y]`\n",
    "<br><br>\n",
    "In fact this calculates **if value function is a good predictor of the returns or if it's just worse than predicting nothing.**\n",
    "ev=0  =>  might as well have predicted zero\n",
    "ev<0  =>  worse than just predicting zero so you're overfitting (need to tune some hyperparameters)\n",
    "ev=1  =>  perfect prediction\n",
    "\n",
    "--> The goal is that **ev goes closer and closer to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(policy,\n",
    "            env,\n",
    "            nsteps,\n",
    "            total_timesteps,\n",
    "            gamma,\n",
    "            lam,\n",
    "            vf_coef,\n",
    "            ent_coef,\n",
    "            lr,\n",
    "            max_grad_norm,\n",
    "            log_interval):\n",
    "\n",
    "    noptepochs = 4\n",
    "    nminibatches = 8\n",
    "\n",
    "\n",
    "    # Get the nb of env\n",
    "    nenvs = env.num_envs\n",
    "\n",
    "    # Get state_space and action_space\n",
    "    ob_space = env.observation_space\n",
    "    ac_space = env.action_space\n",
    "\n",
    "    # Calculate the batch_size\n",
    "    batch_size = nenvs * nsteps # For instance if we take 5 steps and we have 5 environments batch_size = 25\n",
    "\n",
    "    batch_train_size = batch_size // nminibatches\n",
    "\n",
    "    assert batch_size % nminibatches == 0\n",
    "\n",
    "    # Instantiate the model object (that creates step_model and train_model)\n",
    "    model = Model(policy=policy,\n",
    "                ob_space=ob_space,\n",
    "                action_space=ac_space,\n",
    "                nenvs=nenvs,\n",
    "                nsteps=nsteps,\n",
    "                ent_coef=ent_coef,\n",
    "                vf_coef=vf_coef,\n",
    "                max_grad_norm=max_grad_norm)\n",
    "\n",
    "\n",
    "    # Load the model\n",
    "    # If you want to continue training\n",
    "    load_path = \"./models/260/model.ckpt\"\n",
    "    model.load(load_path)\n",
    "\n",
    "    # Instantiate the runner object\n",
    "    runner = Runner(env, model, nsteps=nsteps, total_timesteps=total_timesteps, gamma=gamma, lam=lam)\n",
    "\n",
    "\n",
    "    # Start total timer\n",
    "    tfirststart = time.time()\n",
    "\n",
    "\n",
    "    for update in range(1, total_timesteps//batch_size+1):\n",
    "        # Start timer\n",
    "        tstart = time.time()\n",
    "\n",
    "        # Get minibatch\n",
    "        obs, actions, returns, values = runner.run()\n",
    "\n",
    "        # Here what we're going to do is for each minibatch calculate the loss and append it.\n",
    "        mb_losses = []\n",
    "        total_batches_train = 0\n",
    "\n",
    "        # Index of each element of batch_size\n",
    "        # Create the indices array\n",
    "        indices = np.arange(batch_size)\n",
    "\n",
    "        for _ in range(noptepochs):\n",
    "            # Randomize the indexes\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            # 0 to batch_size with batch_train_size step\n",
    "            for start in range(0, batch_size, batch_train_size):\n",
    "                end = start + batch_train_size\n",
    "                mbinds = indices[start:end]\n",
    "                slices = (arr[mbinds] for arr in (obs, actions, returns, values))\n",
    "                mb_losses.append(model.train(*slices, lr))\n",
    "\n",
    "        # Feedforward --> get losses --> update\n",
    "        lossvalues = np.mean(mb_losses, axis=0)\n",
    "\n",
    "        # End timer\n",
    "        tnow = time.time()\n",
    "\n",
    "        # Calculate the fps (frame per second)\n",
    "        fps = int(batch_size / (tnow - tstart))\n",
    "\n",
    "        if update % log_interval == 0 or update == 1:\n",
    "            \n",
    "            \"\"\"\n",
    "            Computes fraction of variance that ypred explains about y.\n",
    "            Returns 1 - Var[y-ypred] / Var[y]\n",
    "            interpretation:\n",
    "            ev=0  =>  might as well have predicted zero\n",
    "            ev=1  =>  perfect prediction\n",
    "            ev<0  =>  worse than just predicting zero\n",
    "            \"\"\"\n",
    "            ev = explained_variance(values, returns)\n",
    "            logger.record_tabular(\"nupdates\", update)\n",
    "            logger.record_tabular(\"total_timesteps\", update*batch_size)\n",
    "            logger.record_tabular(\"fps\", fps)\n",
    "            logger.record_tabular(\"policy_loss\", float(lossvalues[0]))\n",
    "            logger.record_tabular(\"policy_entropy\", float(lossvalues[2]))\n",
    "            logger.record_tabular(\"value_loss\", float(lossvalues[1]))\n",
    "            logger.record_tabular(\"explained_variance\", float(ev))\n",
    "            logger.record_tabular(\"time elapsed\", float(tnow - tfirststart))\n",
    "            logger.dump_tabular()\n",
    "\n",
    "            savepath = \"./models/\" + str(update) + \"/model.ckpt\"\n",
    "            model.save(savepath)\n",
    "            print('Saving to', savepath)\n",
    "            \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build the play function in model.py\n",
    "- This function will be use to play an environment using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(policy, env):\n",
    "\n",
    "    # Get state_space and action_space\n",
    "    ob_space = env.observation_space\n",
    "    ac_space = env.action_space\n",
    "\n",
    "    # Instantiate the model object (that creates step_model and train_model)\n",
    "    model = Model(policy=policy,\n",
    "                ob_space=ob_space,\n",
    "                action_space=ac_space,\n",
    "                nenvs=1,\n",
    "                nsteps=1,\n",
    "                ent_coef=0,\n",
    "                vf_coef=0,\n",
    "                max_grad_norm=0)\n",
    "    \n",
    "    # Load the model\n",
    "    load_path = \"./models/260/model.ckpt\"\n",
    "    model.load(load_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Play\n",
    "    score = 0\n",
    "    boom = 0\n",
    "    done = False\n",
    "    while done == False:\n",
    "        boom +=1\n",
    "        \n",
    "        # Get the action\n",
    "        actions, values = model.step(obs)\n",
    "        \n",
    "        # Take actions in env and look the results\n",
    "        obs, rewards, done, _ = env.step(actions)\n",
    "        \n",
    "        score += rewards\n",
    "    \n",
    "        env.render()\n",
    "        \n",
    "    \n",
    "    print(\"Score \", score)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Build the agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `config.gpu_options.allow_growth = True` : This creates a GPU session\n",
    "-  `model.learn(policy=policies.A2CPolicy,env=SubprocVecEnv([env.make_train_0, env.make_train_1, env.make_train_2, env.make_train_3, env.make_train_4, env.make_train_5,env.make_train_6,env.make_train_7,env.make_train_8,\n",
    "env.make_train_9,env.make_train_10,env.make_train_11,env.make_train_12 ]), nsteps=2048, # Steps per environment\n",
    "total_timesteps=10000000,gamma=0.99,lam = 0.95,vf_coef=0.5,ent_coef=0.01,lr = 2e-4,max_grad_norm = 0.5, log_interval = 10)` : Here we just call the learn function that contains all the elements needed to train our A2C agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = tf.ConfigProto()\n",
    "\n",
    "    # Avoid warning message errors\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "    # Allowing GPU memory growth\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config):\n",
    "        model.learn(policy=policies.A2CPolicy,\n",
    "                            env=SubprocVecEnv([env.make_train_0, env.make_train_1, env.make_train_2, env.make_train_3, env.make_train_4, env.make_train_5,env.make_train_6,env.make_train_7,env.make_train_8,env.make_train_9,env.make_train_10,env.make_train_11,env.make_train_12 ]), \n",
    "                            nsteps=2048, # Steps per environment\n",
    "                            total_timesteps=10000000,\n",
    "                            gamma=0.99,\n",
    "                            lam = 0.95,\n",
    "                            vf_coef=0.5,\n",
    "                            ent_coef=0.01,\n",
    "                            lr = 2e-4,\n",
    "                            max_grad_norm = 0.5, \n",
    "                            log_interval = 10\n",
    "                            )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
